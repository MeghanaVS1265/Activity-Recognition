# -*- coding: utf-8 -*-
"""loss_guidedActivation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wZM4VsbLgrBXtPJbLhkqdDmdn_Y8fnnV
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
# Using tensorboard to log data
# %load_ext tensorboard

# Input image size for our CNN
pixels = 500

# batch size
BATCH_SIZE = 12

#Loads the previous model or create a new model from scratch
load_model = True
model_path = "/content/drive/MyDrive/CV_Project/15_epochs.h5"

from tensorflow.keras.preprocessing.image import load_img
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras import models
from tensorflow.keras import layers
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import optimizers
from tensorflow.keras.models import load_model
from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input
from sklearn import metrics
import numpy as np
import math

#
# Loads or create our model
#
if (load_model):
    
    model =load_model(model_path)

else :
    #Here, test and training samples are resized to (pixel, pixel, 3)
    conv_base = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(pixels,pixels,3))
    

    from tensorflow.keras import layers
    from tensorflow.keras import Input
    from tensorflow.keras.models import Model

    model_input = Input(shape=(pixels, pixels, 3))
    x = conv_base(model_input)

    # Classification Branch with 1024 kernels and relu function
    class_branch = layers.Conv2D(1024, (3, 3), padding="same", strides=(1,1), activation="relu", name="CAM")(x)
    class_branch = layers.GlobalAveragePooling2D(name="GAP")(class_branch)
    class_output = layers.Dense(40, activation="softmax", name="class")(class_branch)

    # Human Localization Branch split into 4 conv2D layers
    human_branch = layers.Conv2D(512, (3, 3), padding="same", strides=(1,1), activation="relu")(x)
    human_branch = layers.Conv2D(64, (3, 3), padding="same", strides=(1,1), activation="relu")(human_branch)
    human_branch = layers.Conv2D(32, (3, 3), padding="same", strides=(1,1), activation="relu")(human_branch)
    human_output = layers.Conv2D(1, (3, 3), padding="same", strides=(1,1), activation="relu", name="mask")(human_branch)

    # The model
    model = Model(model_input, [class_output, human_output])



model.summary()

from tensorflow.keras.utils import plot_model

plot_model(model, to_file='model_diagram.png', show_shapes=True, show_layer_names=True)

# Check shape of the weights of the final layer
all_amp_layer_weights = model.get_layer("class").get_weights()[0]
all_amp_layer_weights.shape

cam_shape = tuple(model.get_layer("CAM").output.get_shape().as_list()[1:])
cam_shape

mask_shape = tuple(model.get_layer("mask").output.get_shape().as_list()[1:])
mask_shape

import pandas as pd
#convert xml to csv file and add to path
annotations = "/content/drive/MyDrive/CV_Project/Dataset/annotations1.csv"

df = pd.read_csv(annotations)
df

# Dicitonaries created to map from "action class" to "index"
action_classes = sorted(df['action'].unique())

n = 0
class_to_idx = dict()
idx_to_class = dict()

for action in action_classes:
    class_to_idx[action] = n
    n += 1
    
for action, idx in class_to_idx.items():
    idx_to_class[idx] = action

def create_mask(row, mask_height, mask_width):
    # Image height and width
    height, width = row.height, row.width
    
    # Bounding box for the human
    xmin, xmax, ymin, ymax = row.xmin, row.xmax, row.ymin, row.ymax
    
    mask_width_ratio =  float(mask_width / width)
    mask_height_ratio = float(mask_height / height)

    mask_xmin = int(xmin * mask_width_ratio + 0.5)
    mask_xmax = int(xmax * mask_width_ratio + 0.5)
    mask_ymin = int(ymin * mask_height_ratio + 0.5)
    mask_ymax = int(ymax * mask_height_ratio + 0.5)
    
    # Output of the Human Localization Branch is the size(mask_height,mask_width)
    human_mask = np.zeros((mask_height, mask_width, 1))
    human_mask[mask_ymin:mask_ymax, mask_xmin:mask_xmax, 0] = 1

    return human_mask

df['mask'] = df.apply(create_mask, axis=1, mask_height=mask_shape[0], mask_width=mask_shape[1])

df['mask'].to_numpy()[[0,1]].shape

from sklearn.model_selection import train_test_split

train, validation = train_test_split(df, test_size=0.3, stratify=df['action'])


#converted annotations1.csv file:used to generate the class labels for each example. 
from tensorflow.keras.utils import to_categorical
actions = np.array(df['action'].map(class_to_idx))
action_labels = to_categorical(actions)


#This block of code checks the split between train and test images 
#to ensure that the test batch contains 30% of the samples.
stats_df = df[['filename', 'action']].groupby(['action']).count()
stats_df['train'] = train[['filename', 'action']].groupby(['action']).agg(['count'])
stats_df['test'] = validation[['filename', 'action']].groupby(['action']).agg(['count'])
stats_df['sum'] = stats_df['test'] + stats_df['train']
stats_df['ratio'] = stats_df['test'] / stats_df['filename']

stats_df

from tensorflow.keras.preprocessing.image import ImageDataGenerator

image_data_generator = ImageDataGenerator(rescale=1./255)
    
train1_generator = image_data_generator.flow_from_dataframe(
    dataframe=train,
    directory='/content/drive/MyDrive/CV_Project/Dataset/JPEGImages',
    x_col='filename',
    y_col='index',
    class_mode='other',
    target_size=(pixels, pixels),
    batch_size=12  
)

validation1_generator = image_data_generator.flow_from_dataframe(
    dataframe=validation,
    directory='/content/drive/MyDrive/CV_Project/Dataset/JPEGImages',
    x_col='filename',
    y_col='index',
    class_mode='other',
    target_size=(pixels, pixels),
    batch_size=12  
)

# labels
def multilabel_flow_from_dataframe(data_generator):
    for x, y in data_generator:
        # indices will be equal to batch size
        indices = y.astype(np.int).tolist()
        
        # one-hot encoded action labels for this training batch
        action = action_labels[indices]
        
        # generate human mask for this training batch
        masks = df.iloc[indices]['mask'].to_numpy()
        
        mask_batch = np.zeros((len(indices), *mask_shape))
        for i, mask in enumerate(masks):
            mask_batch[i] = mask
        
        yield x, [action, mask_batch]


train_generator = multilabel_flow_from_dataframe(train1_generator)
validation_generator = multilabel_flow_from_dataframe(validation1_generator)

train_steps = math.ceil(len(train.index)/BATCH_SIZE)
validation_steps = math.ceil(len(validation.index)/BATCH_SIZE)

print(train_steps)
print(validation_steps)

from tensorflow.keras.callbacks import ReduceLROnPlateau

# callbacks for saving best model
filepath = "models/loss_guided_activation/checkpoints/epoch_{epoch:02d}-{val_loss:.2f}.h5"
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='min')

# call back for tensorboard
logdir = "models/loss_guided_activation/logs"
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)

model.compile(loss={'class': 'categorical_crossentropy', 'mask': 'mse'},
              loss_weights=[0.5, 0.5], optimizer=optimizers.Adam(lr=1e-4), metrics = ['acc'])


#These variables are added to the training history 
prev_epochs = 0
epochs = 5

# Train the model
history = model.fit_generator(train_generator, steps_per_epoch=100, initial_epoch=prev_epochs, epochs=epochs, 
                              validation_data=validation_generator, validation_steps=100, 
                              callbacks=[tensorboard_callback, checkpoint])

model.save(f"models/loss_guided_activation/15_epochs.h5")

model.summary()

import matplotlib.pyplot as plt

plt.plot(history.history['class_acc'])
plt.plot(history.history['val_class_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'dev'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['class_loss'])
plt.plot(history.history['val_class_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'dev'], loc='upper left')
plt.show()

cam_shape = tuple(model.get_layer("CAM").output.get_shape().as_list()[1:])

# generator
def multiple_outputs(generator, image_dir, batch_size, image_size):
    gen = generator.flow_from_directory(
        image_dir,
        target_size=(image_size, image_size),
        batch_size=batch_size,
        class_mode='categorical', shuffle=False)
    
    while True:
        gnext = gen.next()
        yield gnext[0], gnext[1]

BATCH_SIZE = 12

pixels = 500

valid_datagen = ImageDataGenerator(rescale=1./255)

valid_generator = valid_datagen.flow_from_directory("/content/drive/MyDrive/CV_Project/Dataset/test", batch_size=BATCH_SIZE, target_size=(pixels,pixels), class_mode = 'categorical', shuffle=False)

y_true = valid_generator.classes

# Used for calculating step size for validation set
valid_m = len(valid_generator.classes)

# Get the mapping dictionary from activity index to activity name such as "cooking"
mapping = dict()
for activity, idx in valid_generator.class_indices.items():
    mapping[idx] = activity


valid_steps = math.ceil(valid_m/BATCH_SIZE)
print(valid_steps)

# Simplified evaluation model obtained by removing the human loss mask
evaluation_model = Model(inputs = model.input, outputs = model.get_layer("class").output)
evaluation_model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=1e-4), metrics = ['acc'])

a = evaluation_model.evaluate_generator(valid_generator, valid_steps)
print(a)

# Run the forward pass on our validation set
predictions = evaluation_model.predict_generator(valid_generator, valid_steps)

# Pick the prediction to be the one with the highest probability for each image in the validation set
predictions = predictions.argmax(axis=1)

len(predictions)

# Check out the accuracy for our validation set
(y_true == predictions).mean()

y_true.shape

predictions[0]

matrix = metrics.confusion_matrix(y_true, predictions)

import pandas as pd
import seaborn as sn
import matplotlib.pyplot as plt

# plot the confusion matrix
df_cm = pd.DataFrame(matrix, index = [mapping[i] for i in range(40)], columns = [mapping[i] for i in range(40)])
plt.figure(figsize = (40, 40))
sn.heatmap(df_cm, annot=True)

all_amp_layer_weights=model.get_layer("class").get_weights()[0]

all_amp_layer_weights.shape


final_model = Model(inputs = model.input, 
                  outputs = (model.get_layer("CAM").output, model.get_layer("class").output))

final_model.summary()

from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input

def predict(img_path, model, all_amp_layer_weights):
    
    # Load and preprocess image
    img = image.load_img(img_path, target_size=(pixels,pixels))
    
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    
    processed_input = preprocess_input(x)
    
    # Run model's prediction to output from last Conv Layer + category prediction
    last_conv_output, pred_vec = final_model.predict(processed_input)
    
    last_conv_output = np.squeeze(last_conv_output)

    # category with highest probability
    pred = np.argmax(pred_vec)
    
    scale = pixels / last_conv_output.shape[0]
    filters = last_conv_output.shape[2]
    
    # Rescale to input image size
    mat_for_mult = scipy.ndimage.zoom(last_conv_output, (scale,scale,1), order=1)
    
    # obtain weights associated with the predicted class
    amp_layer_weights = all_amp_layer_weights[:, pred]
    
    # Weighted sum of the activation maps for the predicted class resized back to original image size
    final_output = np.dot(mat_for_mult.reshape((pixels*pixels, filters)), amp_layer_weights).reshape(pixels,pixels)
    
    return final_output, idx_to_class[pred]

import matplotlib.pyplot as plt
import scipy
import glob

from tensorflow.keras.preprocessing import image

#  Images downloaded from web to test activity classification
image_dir = "/content/ImagesFromWeb/"
pattern  = image_dir + "*"
test_images = sorted(glob.glob(pattern))

# Details for output image
columns = 4
rows = math.ceil(len(test_images) / columns)
fig = plt.figure(figsize=(80, 20 * rows))

for i, image_name in enumerate(test_images):

   
    print(".", end = '')
    image_path = image_dir + image_name
    
    ax = fig.add_subplot(rows, columns, i+1)

    # Load and display original image
    img = tf.keras.preprocessing.image.load_img(image_name, target_size=(pixels,pixels))
    plt.imshow(img)
    
    # Run forward pass to get the Class Activity Map and the predicted class
    cam, pred = predict(image_name, model, all_amp_layer_weights)
     # Displays class activation map
    plt.imshow(cam, cmap='jet', alpha=0.5)
    # Set title to that of predicted class
    ax.set_title(pred, fontsize =60)
    
    
plt.show()